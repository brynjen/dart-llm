name: llm_llamacpp
description: llama.cpp backend implementation for LLM interactions. Enables local on-device inference with GGUF models on Android, iOS, macOS, Windows, and Linux.
version: 0.1.0
resolution: workspace
homepage: https://github.com/brynjen/dart-llm
repository: https://github.com/brynjen/dart-llm
issue_tracker: https://github.com/brynjen/dart-llm/issues
documentation: https://pub.dev/documentation/llm_llamacpp/latest/
topics:
  - llamacpp
  - llama
  - llm
  - flutter
  - ffi

environment:
  sdk: ^3.8.0
  flutter: ">=3.24.0"

dependencies:
  flutter:
    sdk: flutter
  llm_core: ^0.1.0
  ffi: ^2.1.3
  http: ^1.3.0
  path: ^1.9.0
  # Native Assets support (for hook/build.dart)
  hooks: ^1.0.0
  code_assets: ^1.0.0
  logging: ^1.3.0

dev_dependencies:
  lints: ^6.0.0
  test: ^1.24.0
  ffigen: ^20.1.1

flutter:
  plugin:
    platforms:
      android:
        ffiPlugin: true
      ios:
        ffiPlugin: true
      macos:
        ffiPlugin: true
      windows:
        ffiPlugin: true
      linux:
        ffiPlugin: true

